{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE-40959: Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "from utils import load_data\n",
    "from models import Dense\n",
    "from train import train\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# The following two lines let us reload external modules in the notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description and Loading Data\n",
    "\n",
    "On this notebook, we are going to work on farsi OCR dataset. As its name implies, it is like famous **MNIST** dataset but it consists of images of handwritten digits in farsi. Each instance of this dataset is 32 * 32 gray-scale image. It is totally composed of 80000 instances. After loading this data, let's plot some images in order to see how they look like.\n",
    "\n",
    "Train, validation and test sets are loaded using a method in `utils.py`. Training set includes 0.7 of the whole dataset and test set just has 0.1 of it. Rest is assigned as validation set.\n",
    "\n",
    "**Note**: Images are flattened that's why their size is 1024 = 32 * 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_validation, y_validation, x_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = 10\n",
    "num_class = 10\n",
    "fig, ax = plt.subplots(nrows=num_sample, ncols=num_class)\n",
    "\n",
    "for i in range(num_class):\n",
    "    class_i_images = [x_train[k] for k in range(x_train.shape[0]) if y_train[k][i] == 1]\n",
    "    for j in range(num_sample):\n",
    "        ax[j, i].get_xaxis().set_visible(False)\n",
    "        ax[j, i].get_yaxis().set_visible(False)\n",
    "        ax[j, i].imshow(class_i_images[j].reshape((32, 32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Flags\n",
    "Tensorflow has the option of setting of some flags which should be defined once and can be used in any file later.\n",
    "\n",
    "Here we define flags for\n",
    "1. `learning_rate`: Shows the initial learning rate in optimization process.\n",
    "\n",
    "2. `num_epoch`: The total number of epochs for training process.\n",
    "\n",
    "3. `weight_decay`: The coeffecient of L2 Loss term in total Loss function.\n",
    "\n",
    "4. `batch_size`: Size of each batch given to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('num_epoch', 10, 'Number of epochs to train.')\n",
    "flags.DEFINE_float('weight_decay', 0., 'Weight for L2 loss')\n",
    "flags.DEFINE_integer('batch_size', 100, 'Size of batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Initializers\n",
    "\n",
    "In the following cell we are importing some intializers which are defined in `utils.py`. Based on their explanation, you have to complete their code in `utils.py` and then import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import glorot_initializer, zero_initializer, normal_initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing MLP Model\n",
    "\n",
    "`Dense` model is defined as a python class in `models.py`. Its constructor receives as input followings:\n",
    "\n",
    "1. num_hidden: Number of hidden units and output units.\n",
    "2. weight_initializer: Function used for initializing weights\n",
    "3. bias_initializer: Function used for initializing biases\n",
    "4. act: Activation function used for hidden layers\n",
    "5. logging: This is a boolean showing whether the model saves log of weights and biases for later visualization using tensorboard.\n",
    "6. stddev: Standard deviation in case of having normal initializer for weights of layers.\n",
    "\n",
    "You have to complete some parts of `__init__`, `_loss`, `_accuracy`, `_log_vars` and `_build` methods in this class.\n",
    "\n",
    "There is also another python file `layers.py` consists of the class `DenseLayer`. The arguments of its constructor are:\n",
    "1. input_dim: Dimension of input to layer\n",
    "2. output_dim: Dimension of output of layer\n",
    "3. act: Activation function of layer\n",
    "4. weight_initializer: Function used for initializing weights\n",
    "5. bias_initializer: Function used for initializing biases\n",
    "6. stddev: Standard deviation in case of having normal initializer for weights of the layer.\n",
    "\n",
    "Some part of `__call__` method of is left for you to complete.\n",
    "\n",
    "**Note**: It is necessary to complete aforementioned python files before moving forward to following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting on features of an MLP\n",
    "\n",
    "In the rest of this notebook, some experiments should be done on different setting like regularization, activation function, number of layers, etc combined with some visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the following cell we have defined a model with number of hidden units 200, 100, 50, 10. Note that the outputs of last 10 units, after which a softmax function is applied, act as scores for 10 class of digits in data.\n",
    "\n",
    "You also have to use uniform glorot initializer and zero initializer for weights and biases, respectively.\n",
    "\n",
    "The last point here is that we are using **sigmoid** as activation function of all layers.\n",
    "\n",
    "Later, we will apply another one and observe its difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dense(num_hidden=[200, 100, 50, 10],\n",
    "              weight_initializer=glorot_initializer,\n",
    "              bias_initializer=zero_initializer,\n",
    "              act=tf.nn.sigmoid,\n",
    "              logging=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model we need to trigger training process by using the code partly prepared in `train.py`. After filling required lines run the following cell to start training.\n",
    "In this file some writers are defined which are later used for plotting visualizations in tensorboard framework. Summary information defined as scalars (like loss) and histograms (like weights) are saved by this writers in `logs` folder near existing files. More specifically, for each model another folder whose name came from `log_file` variable is created.\n",
    "\n",
    "Furthermore, when you define a session using `with`, the session is just restricted to its following context and can not be used in outer scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    log_file = str(model.act.__name__)\n",
    "    train(x_train, y_train, x_validation, y_validation, x_test, y_test, model, sess, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:** In order to run different computation graphs in the same process, it is compulsary to reset the default graph to a new one which is defined later. To do so, you have to use method `tf.rest_default_graph()` as in the following cell. Otherwise, you will end up with lots of nodes in the default graph of computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's maintain the same settings but change activation function to **tanh** and run the whole process again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dense(num_hidden=[200, 100, 50, 10],\n",
    "              weight_initializer=glorot_initializer,\n",
    "              bias_initializer=zero_initializer,\n",
    "              act=tf.nn.tanh,\n",
    "              logging=True)\n",
    "with tf.Session() as sess:\n",
    "    log_file = str(model.act.__name__)\n",
    "    train(x_train, y_train, x_validation, y_validation, x_test, y_test, model, sess, log_file)\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Question 1\n",
    "Compare `tanh` and `sigmoid` based on above results. Explain your observation from the visualizations produced by tensorboard.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Initializations\n",
    "\n",
    "Next we want to see the effect of aforementioned initializers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we use zero initializer both for biases and weights. By doing so, specially for weight initialization, the network will get in trouble with breaking the symmetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dense(num_hidden=[200, 50, 10],\n",
    "              weight_initializer=zero_initializer,\n",
    "              bias_initializer=zero_initializer,\n",
    "              act=tf.nn.tanh,\n",
    "              logging=True)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    log_file = str(model.act.__name__) + \"_\" + str(model.weight_initializer.__name__)\n",
    "    train(x_train, y_train, x_validation, y_validation, x_test, y_test, model, sess, log_file)\n",
    "tf.reset_default_graph()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Question 2\n",
    "\n",
    "Use tensorboard visualizations of weights and learning curves (like loss and accuracy) as well to discuss about the issue raised by using zeros initializer for weights.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what is the impact of using normal initializer for weights.\n",
    "\n",
    "Do not forget to send in standard deviation of gaussain distribution used for sampling weights as an argument to model constructor for all next experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stddev_list = [10, 1, 0.1]\n",
    "for stddev in stddev_list:\n",
    "    print('Normal Initializer with stddev: {}'.format(stddev))\n",
    "    model = Dense(num_hidden=[200, 50, 10],\n",
    "                  weight_initializer=normal_initializer,\n",
    "                  bias_initializer=zero_initializer,\n",
    "                  act=tf.nn.tanh,\n",
    "                  logging=True,\n",
    "                  stddev=stddev)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        log_file = str(model.act.__name__) + \"_\" + str(model.weight_initializer.__name__) + \"_\" + str(stddev)\n",
    "        train(x_train, y_train, x_validation, y_validation, x_test, y_test, model, sess, log_file)\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Question 3\n",
    "\n",
    "Use tensorboard visualizations of weights and learning curves (like loss and accuracy) as well to describe differences through training process caused by using different standard deviations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Architectures (layers and units)\n",
    "\n",
    "Next, we will cast light on the importance of architecture of neural networks (more specifically number of layers and units)\n",
    "\n",
    "Here we will examine two architectures. Comparing with above setting, we will increase the number of epochs to 15 and change learning rate to 0.01. We also use normal initializer for weights with `stddev=1` and `tanh` as activation function. What makes difference between two architectures is the number of layers and units.\n",
    "\n",
    "In the first one we have 3 hidden layers with 500, 100, 50 units, respectively. This model imposes high computational cost. On the other hand, the second network has one less hidden layer but same number of units in the rest of layers. Totally it has less complexity in terms of number of parameters compared with the first network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS.num_epoch = 15\n",
    "FLAGS.learning_rate = 0.01\n",
    "num_hidden_list = [[500, 100, 50, 10], [100, 50, 10]]\n",
    "stddev = 1\n",
    "for num_hidden in num_hidden_list:\n",
    "    print('Number of hidden units: ', num_hidden)\n",
    "    model = Dense(num_hidden=num_hidden,\n",
    "                  weight_initializer=normal_initializer,\n",
    "                  bias_initializer=zero_initializer,\n",
    "                  act=tf.nn.tanh,\n",
    "                  logging=True,\n",
    "                  stddev=stddev)\n",
    "    with tf.Session() as sess:\n",
    "        log_file = str(model.act.__name__) + \"_\" + str(model.num_hidden)\n",
    "        train(x_train, y_train, x_validation, y_validation, x_test, y_test, model, sess, log_file)\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 4\n",
    "By using tensorboard visualiztions justify the different outcomes of training the above two neural network architectures.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Another important point to consider is applying regularization and being aware of its impact on training and generalization. Its most important rule is to prevent our learning process from overfitting (you may have encountered this phenomenon above!)\n",
    "\n",
    "In first experiment L2 regularization technique is not applied. However, in rest of them, it is used for penalizing two sets of weights:\n",
    "\n",
    "1. Weights between input and first hidden layer.\n",
    "2. Weights between first and second hidden layer.\n",
    "\n",
    "Another point to mention is that here we use `tanh` activation function for all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay_list = [0., 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "for weight_decay in weight_decay_list:\n",
    "    print('Weight decay of: {}'.format(weight_decay))\n",
    "    FLAGS.weight_decay = weight_decay \n",
    "    model = Dense(num_hidden=[500, 100, 50, 10],\n",
    "                  weight_initializer=normal_initializer,\n",
    "                  bias_initializer=zero_initializer,\n",
    "                  act=tf.nn.tanh,\n",
    "                  logging=True,\n",
    "                  stddev=1)\n",
    "    with tf.Session() as sess:\n",
    "        log_file = str(model.act.__name__) + \"_\" + str(FLAGS.weight_decay)\n",
    "        train(x_train, y_train, x_validation, y_validation, x_test, y_test, model, sess, log_file)\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 5\n",
    "By using visualiztion of learning curves (specially loss curve) try to explain the impact that L2 regularization had on training process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Note**: \n",
    "So far, some questions were placed between cells of codes and descriptions. \n",
    "\n",
    "In addition to **completing the code files**, please send a **report** including your answer to these questions as well. Do not forget to put the diagrams and visualizations needed in each part."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
